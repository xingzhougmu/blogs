# 神经网络学习笔记 - A toy example
**资料来源**：[CS231 Convolutional Neural Network for Visual Recognition](http://cs231n.github.io/) - [Fei-fei Li](http://vision.stanford.edu/feifeili/)
## 分类问题

分类是指识别出样本所属的类别。根据识别前是否需要进行模型训练，可分为有监督分类和无监督分类。

- 有监督分类：根据已知训练样本，通过计算选择特征参数，建立判别函数以对样本进行的分类。
- 无监督分类：在毫无先验知识的条件下，仅凭数据，即自然聚类的特性进行的分类。

下面以图像分类问题为例，进一步解释分类问题。

图像分类器针对输入的图像，输出图像所属的类别或者分属每个类别的可能性（例如猫，狗，帽子，茶杯等）。值得一提的是：对于计算机来说，它所看到的图像是一个三维矩阵。以下图为例，这张猫的图像长度是248个像素，宽度是400个像素，有三个颜色通道（红蓝绿）。

![](./img/NN-classify.png)

尽管对于人尤其是成年人来说，要识别出图片中物体的类别简直是小菜一碟。但对于计算机来说，要通过某个算法达到成年人的水平还是相当有挑战的。而且该算法还必须具备相当高的鲁棒性，能够克服图片的旋转问题，遮挡问题，柔性变形问题等（如下图所示）。

![](./img/NN-challenges.jpeg)

在思考如何才能赋予计算机识别物体的能力之前，我们先看看我们自己是如何获得辨别物体的技能的。通常有下面两种方式：

1. 物体会在我们大脑中留下印象（image + category）。再次碰到时，我们会跟大脑中的印象进行比对，挑选出最相似的那个并把它们归为同一类；
2. 我们会对看到的物体进行特征总结，并在脑海中形成该物体的抽象模型。再次碰到时会根据该模型进行判断。

回到计算机分类问题，上述两种方式可以总结为：

1. 聚类问题
2. 分类器学习问题

### K-近邻分类器
近邻分类器虽然非常简单，但在实际应用中极少采用。如果在某些场景下试图采用K-近邻分类器，请注意以下几点：

1. 对数据进行预处理：将特征数据进行标准化（0均值，单位协方差）。
2. 如果特征向量的维数很大，进行降维处理。比如PCA。
3. 将训练样本随机拆分成 training set 和 validation set。
4. 在 validation set 上进行 KNN 分类器的评估以选择合适的K以及相似度度量。
5. 如果KNN分类器运行效率过低，可以考虑近似近邻分类器，如[FLANN](http://www.cs.ubc.ca/research/flann/)。

## 线性分类器
尽管K-近邻分类器比较简单，但它有许多缺点：

- 训练器必须保存所有的训练数据；
- 在对测试样本进行分类的时候，计算量很大。

而线性分类器可以很好的克服这些缺点。一般来说，线性分类器有两个主要组成部分：目标函数（Score function），以及损失函数（loss function）。

### 目标函数
目标函数实现图像像素值到每个类别确信值的映射。例如训练集为图像集，x_{i} 

When \\( a \ne 0 \\), there are two solutions to \\(ax^2 + bx + c = 0\\) and they are:
$$ x = {-b \pm \sqrt{b^2-4ac} \over 2a} $$




### 损失函数

## 下降梯度法

## 反向传播法

## 神经网络模型

## 神经网络模型 - A Toy Example

## 卷积神经网络
